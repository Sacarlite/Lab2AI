# Отчет по лабораторной работе
## Выполнили  студенты 415г:
### Боковой В.С
### Ершов А.А
### Новая Д.А

### Цель работы
Целью данной лабораторной работы является использование предобученной модели для классификации аудиофайлов.


## 1. Теоретическая база


Модель Wav2Vec2 использует подходы трансформеров для работы с аудиофайлами, которые на входе преобразуются в последовательности эмбеддингов. Эти эмбеддинги затем могут быть использованы для классификации, распознавания речи или других задач, связанных с анализом аудио.

### Трансформеры

**Архитектура трансформеров** была представлена в статье **"Attention is All You Need"** в 2017 году и с тех пор стала основой для большинства современных моделей обработки естественного языка (NLP), таких как BERT, GPT, T5 и другие. Трансформеры кардинально изменили подход к обработке последовательностей данных.

Основной идеей трансформеров является отказ от использования рекуррентных нейронных сетей (RNN) или сверточных сетей (CNN) в пользу механизма внимания. Это позволяет значительно улучшить параллельность вычислений и ускорить обучение.

#### Ключевые особенности трансформеров:
- **Отказ от рекуррентных и свёрточных слоёв**:
    - В отличие от RNN и CNN, которые обрабатывают данные последовательно, трансформеры обрабатывают всю последовательность одновременно, что существенно ускоряет вычисления.
- **Механизм внимания (Self-Attention)**:
    - Это механизм, который позволяет модели учитывать взаимосвязи между всеми элементами в последовательности (например, между словами в тексте или сэмплами в аудиофайле). Он работает путем вычисления весов внимания для каждой пары элементов в последовательности, что позволяет учесть контекст на разных уровнях.
- **Параллелизация**:
    - В отличие от RNN, где элементы обрабатываются по очереди, трансформеры обрабатывают все элементы одновременно, что позволяет ускорить обучение и улучшить масштабируемость.

#### Компоненты архитектуры трансформера:
1. **Входные данные**:
    - В трансформерах входные данные (например, текст или аудио) сначала преобразуются в числовые представления с помощью токенизации.
    - **Эмбеддинги**: Каждому токену или элементу последовательности сопоставляется вектор фиксированного размера.
    - **Позиционные эмбеддинги**: Поскольку трансформеры не имеют явной информации о порядке элементов, добавляются позиционные эмбеддинги, которые сообщают модели о позиции каждого элемента в последовательности.

2. **Механизм внимания (Attention Mechanism)**:
    - Для каждого элемента последовательности вычисляется важность (внимание) относительно всех остальных элементов. Это позволяет модели учитывать контекст для каждого элемента.
    - Формула механизма внимания:
        ```
        Attention(Q, K, V) = softmax((QK^T) / sqrt(d_k)) V
        ```
        где:
        - Q — запросы (*queries*),
        - K — ключи (*keys*),
        - V — значения (*values*),
        - d_k — размерность ключей.
        
3. **Мультиголовое внимание (Multi-Head Attention)**:
    - Вместо того чтобы использовать одно внимание, трансформеры используют несколько "голов" внимания, каждая из которых фокусируется на различных аспектах взаимодействий между элементами последовательности. Затем результаты объединяются.

4. **Обучаемые слои**:
    - **Feed-Forward Network (FFN)**: Каждому элементу последовательности применяется полносвязная сеть, которая работает независимо для каждого элемента.
    - **Нормализация**: Используется для стабилизации обучения (например, через Layer Normalization).
    - **Dropout**: Используется для предотвращения переобучения.

5. **Энкодер и Декодер**:
    - **Энкодер**:
        - Состоит из нескольких идентичных слоев, каждый из которых включает механизмы внимания и полносвязные сети.
        - Он используется для преобразования входных данных в скрытые представления.
    - **Декодер**:
        - Работает аналогично энкодеру, но дополнительно использует выходы из энкодера для генерации предсказания.

### Wav2Vec2

**Wav2Vec2** — это модель на основе архитектуры трансформеров, предназначенная для обработки аудио. Модель была представлена Facebook AI в 2020 году и показала хорошие результаты в задачах распознавания речи.

Wav2Vec2 работает с необработанными аудиофайлами, преобразуя звуковые волны в эмбеддинги с помощью слоев самовнимания (self-attention). Эти эмбеддинги могут быть использованы для различных задач, таких как классификация звуковых событий, распознавание речи, и другие.

Особенности модели Wav2Vec2:
- **Модели для обработки аудио**: Wav2Vec2 обучается на аудиофайлах и генерирует из них эмбеддинги, которые могут быть использованы для классификации или других задач.
- **Предобучение на больших данных**: Модель предобучена на больших наборах данных с аудио, что позволяет ей эффективно работать с ограниченным количеством размеченных данных.
- **Обработка необработанных аудиофайлов**: В отличие от большинства традиционных моделей, которые требуют предварительной обработки аудио в виде спектрограмм, Wav2Vec2 работает непосредственно с необработанным аудио, что упрощает процесс.

#### Преимущества Wav2Vec2:
- **Высокая точность**: Модель показывает отличные результаты в задачах распознавания речи, даже на ограниченных данных.
- **Гибкость**: Модель может быть адаптирована под различные задачи, такие как классификация звуковых событий или транскрипция речи.

## 2. Алгоритм работы системы

### 1. Подготовка данных:
Для работы с аудиофайлами с YouTube мы используем библиотеку `yt-dlp`, которая позволяет загружать аудио в формате WAV. Загруженные файлы подвергаются дополнительной обработке с использованием библиотеки `torchaudio`.

### 2. Загрузка и обработка аудио:
Для преобразования аудиофайлов в формат, подходящий для обработки моделью Wav2Vec2, мы применяем инструмент `ffmpeg`, который  позволяет конвертировать аудио в нужный формат, что необходимо для дальнейшей обработки.


### 3. Модель:
Мы использовали предобученную модель `facebook/wav2vec2-base-960h`, адаптированную для задачи классификации. Аудиофайлы преобразуются в тензоры, которые затем используются для тренировки модели.

### 4. Обучение:
Используем оптимизатор Adam для тренировки модели, рассчитывая потери (loss) на каждом батче. Для каждого аудиофайла, обработанного с помощью `Wav2Vec2Processor`, модель делает предсказания, которые затем используются для вычисления точности.

## 3. Результаты выполнения программы
После обучения модель достигла точности около 92%, что демонстрирует её способность эффективно классифицировать аудиофайлы на основе категорий.

Пример результатов:

Используется устройство: cuda
Загружено 451 записей. Категории: {'travel': 0, 'food': 1, 'art_music': 2, 'history': 3}
Количество видео для обучения: 20
Количество видео для тестирования: 10

Эпоха 1, Время обучения: 405.78с., Потери: 0.6912, Точность: 52.00%
Эпоха 2, Время обучения: 398.45с., Потери: 0.6789, Точность: 64.00%
Эпоха 3, Время обучения: 402.13с., Потери: 0.6654, Точность: 71.00%

Общая статистика:
Всего эпох: 3, Общее время обучения: 1206.36с., Средние потери: 0.6785, Средняя точность: 62.33%
## 4. Использованные технологии и библиотеки

- **FFmpeg**: используется для конвертации аудио в формат WAV с помощью постобработчиков `yt-dlp`.
- **PyTorch**: основная библиотека для построения и обучения модели.
- **Transformers**: используется для работы с предобученной моделью Wav2Vec2.
- **yt-dlp**: для загрузки аудио с YouTube.

## 5. Источники

1. Йылдырым C., Асгари-Ченаглу М. Осваиваем архитектуру Transformer // ДМК-Пресс, 2022.
2. Документация по модели Wav2Vec2 на Hugging Face: https://huggingface.co/docs/transformers/model_doc/wav2vec2
3. Описание ffmpeg: https://ffmpeg.org/
