# Отчет по лабораторной работе
## Выполнили  студенты 415г:
### Ершов А.А
### Боковой В.С
### Новая Д.А

### Цель работы
Целью данной лабораторной работы является использование предобученной модели для классификации аудиофайлов.


## 1. Теоретическая база

В данной работе для классификации аудиофайлов используется предобученная модель Wav2Vec2, основанная на архитектуре трансформеров. Эта модель преобразует необработанные звуковые волны в эмбеддинги, которые затем применяются для задачи классификации с помощью многослойного перцептрона (MLP). Ниже рассмотрены основные принципы работы трансформеров и модели Wav2Vec2.

### Трансформеры

Архитектура трансформеров была представлена в статье **"Attention is All You Need"** (Vaswani et al., 2017) и стала революционным подходом в обработке последовательностей данных, заменив рекуррентные нейронные сети (RNN) и сверточные сети (CNN) в задачах обработки естественного языка (NLP) и аудио. Трансформеры широко используются в современных моделях, таких как BERT, GPT и Wav2Vec2.

#### Основные особенности трансформеров:
- **Механизм внимания (Self-Attention):**  
  Трансформеры обрабатывают последовательности данных (например, слова в тексте или сэмплы в аудио) с помощью механизма самовнимания. Этот механизм позволяет модели учитывать взаимосвязи между всеми элементами последовательности одновременно, вычисляя веса внимания для каждой пары элементов. Формула внимания:
Attention(Q, K, V) = softmax((QK^T) / √d_k) V
где:
- \( Q \) — запросы (queries),
- \( K \) — ключи (keys),
- \( V \) — значения (values),
- \( d_k \) — размерность ключей (для нормализации).

- **Параллельная обработка:**  
В отличие от RNN, которые обрабатывают данные последовательно, трансформеры анализируют всю последовательность сразу, что значительно ускоряет вычисления и делает их масштабируемыми для больших данных.

- **Отказ от рекуррентности:**  
Трансформеры не используют рекуррентные или сверточные слои, полагаясь исключительно на внимание и полносвязные сети, что упрощает архитектуру и обучение.

#### Структура трансформера:
1. **Входные данные и эмбеддинги:**  
 - Входные данные (текст или аудио) преобразуются в числовые векторы фиксированной размерности — эмбеддинги.
 - Для учета порядка элементов добавляются **позиционные эмбеддинги**, так как трансформеры не имеют встроенного понятия последовательности.

2. **Мультиголовое внимание (Multi-Head Attention):**  
 - Механизм внимания разделяется на несколько "голов", каждая из которых фокусируется на разных аспектах данных. Результаты объединяются для получения более богатого представления.

3. **Полносвязные слои (Feed-Forward Network, FFN):**  
 - После внимания к каждому элементу применяется независимая полносвязная сеть с функцией активации (например, ReLU).

4. **Нормализация и регуляризация:**  
 - Используется **Layer Normalization** для стабилизации обучения и **Dropout** для предотвращения переобучения.

5. **Энкодер и декодер:**  
 - Энкодер преобразует входные данные в скрытые представления.
 - Декодер (при наличии) генерирует выходные данные, опираясь на выходы энкодера. В Wav2Vec2 используется только энкодер для задач извлечения признаков.

Трансформеры эффективны благодаря высокой параллельности и способности моделировать долгосрочные зависимости в данных, что делает их подходящими для обработки как текста, так и аудио.

### Wav2Vec2

**Wav2Vec2** — это модель, разработанная Facebook AI в 2020 году, которая адаптирует архитектуру трансформеров для работы с аудиоданными. Она предназначена для задач обработки звука, таких как распознавание речи и классификация звуковых событий. В данной работе Wav2Vec2 используется для извлечения эмбеддингов из аудиофайлов, которые затем передаются в классификатор.

#### Принципы работы Wav2Vec2:
- **Входные данные:**  
Wav2Vec2 принимает необработанные звуковые волны (waveforms) с частотой дискретизации 16000 Гц. В отличие от традиционных подходов, требующих предварительного извлечения признаков (например, спектрограмм), модель работает напрямую с сырыми аудиоданными.

- **Предобучение:**  
Модель предобучена на больших наборах неразмеченных аудиоданных с использованием самонастраиваемого обучения (self-supervised learning). Это позволяет ей извлекать информативные представления даже при ограниченном количестве размеченных данных.

- **Архитектура:**  
1. **Сверточный слой:**  
   На входе аудио проходит через сверточные слои, которые преобразуют звуковые волны в последовательность признаков.
2. **Трансформер:**  
   Затем признаки обрабатываются блоками трансформеров (энкодерами), использующими самовнимание для создания контекстуальных эмбеддингов.
3. **Выход:**  
   Итоговые эмбеддинги представляют собой усредненные скрытые состояния трансформера (`last_hidden_state.mean(dim=1)`), которые содержат информацию о содержимом аудио.

#### Преимущества Wav2Vec2:
- **Гибкость:**  
Эмбеддинги могут быть адаптированы для различных задач, таких как классификация или распознавание речи, путем добавления дополнительных слоев (например, MLP).
- **Эффективность:**  
Предобучение на больших данных позволяет модели хорошо работать даже с небольшими наборами данных для конкретной задачи.
- **Простота обработки:**  
Работа с необработанным аудио устраняет необходимость сложной предварительной обработки.

## 2. Использование в работе:
В данной лабораторной работе используется предобученная модель `facebook/wav2vec2-base-960h`. Она применяется для извлечения эмбеддингов из аудиофайлов, загруженных с YouTube. Эмбеддинги затем подаются в многослойный перцептрон (MLP), который обучается классифицировать аудио по категориям (например, 'travel', 'food', 'art_music', 'history'). Сама модель Wav2Vec2 не дообучается, а используется в режиме оценки (`eval`), что позволяет эффективно применять её знания, полученные на этапе предобучения.

Таким образом, комбинация трансформеров и Wav2Vec2 обеспечивает мощный инструмент для анализа аудиоданных, сочетая высокую точность и удобство применения.
## 3. Результаты выполнения программы
После обучения модель достигла точности около 90%, что демонстрирует её способность эффективно классифицировать аудиофайлы на основе категорий.

Пример результатов:

Используется устройство: cpu
Распределение классов: {'art_music': 151, 'travel': 108, 'food': 108, 'history': 84}
Загружено 451 записей. Категории: {'travel': 0, 'food': 1, 'art_music': 2, 'history': 3}
Количество видео для обучения: 100
Количество видео для тестирования: 20

Epoch 1/3, Loss: 1.4232, Accuracy: 50.00%
Epoch 2/3, Loss: 0.6541, Accuracy: 80.00%
Epoch 3/3, Loss: 0.3961, Accuracy: 90.00%

Лучшая точность классификатора: 90.00%

## 4. Использованные технологии и библиотеки

- **FFmpeg**: используется для конвертации аудио в формат WAV с помощью постобработчиков `yt-dlp`.
- **PyTorch**: основная библиотека для построения и обучения модели.
- **Transformers**: используется для работы с предобученной моделью Wav2Vec2.
- **yt-dlp**: для загрузки аудио с YouTube.

## 5. Источники

1. Йылдырым C., Асгари-Ченаглу М. Осваиваем архитектуру Transformer // ДМК-Пресс, 2022.
2. Документация по модели Wav2Vec2 на Hugging Face: https://huggingface.co/docs/transformers/model_doc/wav2vec2
3. Описание ffmpeg: https://ffmpeg.org/
